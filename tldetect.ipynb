{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLdetect\n",
    "\n",
    "An implementation of a convolutional neural network to detect the state of traffic lights in images. The states are red, green, yellow and no traffic light. Using the **tensorflow** framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import src.data_processing as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 100\n",
    "BATCH_SIZE = 64\n",
    "KERNEL_SIZE = (3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "Loading the images and labels and treating the dataset to fit better on our model. Note that the images won't be loaded right now, only their file names, in order to save memory and avoid bottlenecks and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels loaded: 13063\n"
     ]
    }
   ],
   "source": [
    "# Loading the labels\n",
    "# At this moment, the conv net will only classify if an image has or not a traffic light.\n",
    "labels = data.load_labels()\n",
    "print(\"Number of labels loaded: %d\" % len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set of 13063 images split in 3 sets.\n",
      "\n",
      "Train: (9438,)\n",
      "Validation: (1665,)\n",
      "Test: (1960,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the images. In reality, just their names\n",
    "images = labels['file']\n",
    "X_train, X_val, X_test = data.split_dataset(images)\n",
    "\n",
    "# Debug\n",
    "print(\"Data set of %d images split in 3 sets.\\n\" % len(images))\n",
    "print(\"Train: {}\".format(X_train.shape))\n",
    "print(\"Validation: {}\".format(X_val.shape))\n",
    "print(\"Test: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lables extracted.\n",
      "\n",
      "Train labels shape: (9438,)\n",
      "Train validation shape: (1665,)\n",
      "Train test shape: (1960,)\n"
     ]
    }
   ],
   "source": [
    "# Spliting the labels\n",
    "y_train = data.extract_labels(labels, X_train)\n",
    "y_val = data.extract_labels(labels, X_val)\n",
    "y_test = data.extract_labels(labels, X_test)\n",
    "\n",
    "# Debug\n",
    "print(\"Lables extracted.\\n\")\n",
    "print(\"Train labels shape: {}\".format(y_train.shape))\n",
    "print(\"Train validation shape: {}\".format(y_val.shape))\n",
    "print(\"Train test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "At the following cells we will define our model. Our model, at this moment, will just classify if a given image has a traffic light or not.\n",
    "\n",
    "Note that the inputs pass through two max-pooling layers before starts to be recognized. This technique is used to reduce the dimensionality of the inputs.\n",
    "\n",
    "The original inputs has the dimension of (1200, 1920, 3).\n",
    "\n",
    "#### The following architecture will be used:\n",
    "1. **Pooling layer**\n",
    "+ **Pooling layer** (Output dimension will be (300, 480, 3))\n",
    "+ **Convolutional layer**\n",
    "+ **Pooling layer**\n",
    "+ **Convolutional layer**\n",
    "+ **Pooling layer**\n",
    "+ **Affine layer**\n",
    "+ **Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.keras import layers, models, backend\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.MaxPool2D(pool_size=2, strides=2,\n",
    "                           data_format='channels_last',\n",
    "                           input_shape=(1200, 1920, 3)\n",
    "                          ))\n",
    "model.add(layers.MaxPool2D(pool_size=2, strides=2))\n",
    "model.add(layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=KERNEL_SIZE, strides=1,\n",
    "                        padding='same',\n",
    "                        activation='relu'\n",
    "                       ))\n",
    "model.add(layers.MaxPool2D(pool_size=2, strides=2))\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=KERNEL_SIZE, strides=1,\n",
    "                        padding='same',\n",
    "                        activation='relu'\n",
    "                       ))\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=KERNEL_SIZE, strides=1,\n",
    "                        padding='same',\n",
    "                        activation='relu'\n",
    "                       ))\n",
    "model.add(layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.keras import datasets\n",
    "from tensorflow.contrib.keras import layers, models, backend\n",
    "\n",
    "(imgs_cif, imgs_labels), (_, _) = datasets.cifar10.load_data()\n",
    "imgs_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.keras import utils, optimizers\n",
    "\n",
    "imgs_labels = utils.to_categorical(imgs_labels, 10)\n",
    "print(\"Data shape: {}, and labels: {}\".format(imgs_cif[0].shape, imgs_labels.shape))\n",
    "print(imgs_labels[0])\n",
    "\n",
    "# Architecture\n",
    "model_cif = models.Sequential()\n",
    "model_cif.add(layers.Conv2D(32, (3, 3), padding='same', input_shape=imgs_cif.shape[1:]))\n",
    "model_cif.add(layers.Activation('relu'))\n",
    "model_cif.add(layers.Conv2D(32, (3, 3)))\n",
    "model_cif.add(layers.Activation('relu'))\n",
    "model_cif.add(layers.MaxPool2D(pool_size=2))\n",
    "\n",
    "model_cif.add(layers.Conv2D(64, (3, 3), padding='same'))\n",
    "model_cif.add(layers.Activation('relu'))\n",
    "model_cif.add(layers.Conv2D(64, (3, 3), padding='same'))\n",
    "model_cif.add(layers.Activation('relu'))\n",
    "model_cif.add(layers.MaxPool2D(pool_size=2))\n",
    "\n",
    "model_cif.add(layers.Flatten())\n",
    "model_cif.add(layers.Dense(512, activation='relu'))\n",
    "model_cif.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "opt = optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "model_cif.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_cif.fit(imgs_cif, imgs_labels, batch_size=BATCH_SIZE, epochs=NUM_ITERATIONS, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/4 [=====================>........] - ETA: 6s - loss: 10.1859 - acc: 0.3646 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/python/keras/engine/training.py\", line 615, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "StopIteration\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-183d18a513c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#                    int(X_train.shape[0]/BATCH_SIZE),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_ITERATIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                    )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/python/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/keras/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1855\u001b[0m             raise ValueError('output of generator should be '\n\u001b[1;32m   1856\u001b[0m                              \u001b[0;34m'a tuple `(x, y, sample_weight)` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1857\u001b[0;31m                              'or `(x, y)`. Found: ' + str(generator_output))\n\u001b[0m\u001b[1;32m   1858\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_output\u001b[0m  \u001b[0;31m# pylint: disable=unpacking-non-sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: None"
     ]
    }
   ],
   "source": [
    "report = model.fit_generator(data.images_generator(X_train[:BATCH_SIZE*4], y_train[:BATCH_SIZE*4], BATCH_SIZE),\n",
    "#                    int(X_train.shape[0]/BATCH_SIZE),\n",
    "                    4,\n",
    "                    epochs=NUM_ITERATIONS\n",
    "                   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
